{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pip==23.2\n",
    "%pip install gym-retro\n",
    "%pip install setuptools==65.5.0 \"wheel<0.40.0\"\n",
    "%pip install gym==0.21.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import retro to play Street Fighter using a ROM\n",
    "import retro\n",
    "# Import time to slow down game\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m retro.import . # Run this from the roms folder, or where you have your game roms\n",
    "!python -m retro.import ./ROMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Environment\n",
    "## What we are going to do! FUNNN\n",
    "\n",
    "- Observation Preprocess - grayscale (DONE), frame delta, resize the frame so we have less pixels (DONE)\n",
    "- Filter the action - parameter DONE\n",
    "- Reward function - set this to the score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install opencv-python\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import environment base class for a wrapper \n",
    "from gym import Env \n",
    "# Import the space shapes for the environment\n",
    "from gym.spaces import Discrete, MultiBinary, Box\n",
    "# Import numpy to calculate frame delta \n",
    "import numpy as np\n",
    "# Import opencv for grayscaling\n",
    "import cv2\n",
    "# Import matplotlib for plotting the image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. frame\n",
    "# 2. preprocess 200x256x3 -> 84x84x1\n",
    "# 3. change in pixels: current_frame-last_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is reward 2 (health, wins, and score)\n",
    "class StreetFighter(Env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Constants for reward calculation\n",
    "        self.START_HEALTH = 176  # Starting health in Street Fighter II\n",
    "        self.ROUND_WIN_MULTIPLIER = 2\n",
    "        self.ROUND_LOSS_MULTIPLIER = -1\n",
    "        \n",
    "        self.observation_space = Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(84, 84, 3),  # Keep 3 channels for RGB\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "        self.action_space = MultiBinary(12)  # type of actions that can be taken\n",
    "        self.game = retro.make(\n",
    "            game='StreetFighterIISpecialChampionEdition-Genesis',\n",
    "            use_restricted_actions=retro.Actions.FILTERED\n",
    "        )\n",
    "        \n",
    "        # Initialize health tracking\n",
    "        self.enemy_health = self.START_HEALTH\n",
    "        self.agent_health = self.START_HEALTH\n",
    "        self.score = 0\n",
    "\n",
    "    def reset(self):\n",
    "        obs = self.game.reset()\n",
    "        obs = self.preprocess(obs)\n",
    "        self.previous_frame = obs\n",
    "        \n",
    "        # Reset game state variables\n",
    "        self.score = 0\n",
    "        self.enemy_health = self.START_HEALTH\n",
    "        self.agent_health = self.START_HEALTH\n",
    "        \n",
    "        return obs\n",
    "    \n",
    "    def preprocess(self, observation):\n",
    "        # Resize first to reduce computation\n",
    "        resized = cv2.resize(observation, (84, 84), interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        # Simple color quantization using bitwise operations\n",
    "        # Reduce to 3 bits per channel (8 values per channel)\n",
    "        quantized = resized & 0b11100000\n",
    "        \n",
    "        # Optional: Create more distinct colors by increasing contrast\n",
    "        # This helps make different elements more distinguishable\n",
    "        quantized = cv2.convertScaleAbs(quantized, alpha=1.2, beta=10)\n",
    "        \n",
    "        return quantized\n",
    "        \n",
    "    def calculate_reward(self, info):\n",
    "        reward = 0\n",
    "\n",
    "        # 1. Score reward\n",
    "        reward += (info['score'] - self.score) * .1\n",
    "\n",
    "        # 2. Round outcome rewards with health\n",
    "        if info['enemy_health'] <= 0:  # Victory\n",
    "            health_ratio = self.agent_health / self.START_HEALTH\n",
    "            reward += self.ROUND_WIN_MULTIPLIER * health_ratio\n",
    "        elif info['health'] <= 0:  # Loss\n",
    "            health_ratio = (info['enemy_health'] / self.START_HEALTH)\n",
    "            reward += self.ROUND_LOSS_MULTIPLIER * health_ratio\n",
    "\n",
    "        return reward\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, _, done, info = self.game.step(action)\n",
    "        obs = self.preprocess(obs)\n",
    "        \n",
    "        reward = self.calculate_reward(info)\n",
    "        \n",
    "        # Update health tracking\n",
    "        self.enemy_health = info['enemy_health']\n",
    "        self.agent_health = info['health']\n",
    "        self.score = info['score']\n",
    "        \n",
    "        return obs, reward, done, info\n",
    "    \n",
    "    def render(self, *args, **kwargs):\n",
    "        self.game.render()\n",
    "\n",
    "    def close(self):\n",
    "        self.game.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/get-started/locally/  <- use this site to download pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu121\n",
    "# these specific versions were needed for cuda "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install stable-baselines3[extra]==1.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the optimzation frame - HPO\n",
    "import optuna\n",
    "# PPO algo for RL\n",
    "from stable_baselines3 import PPO, A2C, DQN\n",
    "# Bring in the eval policy method for metric calculation\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "# Import the sb3 monitor for logging \n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "# Import the vec wrappers to vectorize and frame stack\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
    "# Import os to deal with filepaths\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = './logs/'\n",
    "OPT_DIR = './opt/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative version to use later to bypass factor 64 error\n",
    "\n",
    "# PPO optimization parameters\n",
    "PPO_PARAMS = {\n",
    "    # 1. Core Parameters\n",
    "    'n_steps_range': (2048, 4096),             \n",
    "    'gamma_range': (0.95, 0.9999),               \n",
    "    'learning_rate_range': (5e-8, 1e-6),        \n",
    "    'clip_range_range': (0.1, 0.3),           \n",
    "    'gae_lambda_range': (0.9, 0.98),             \n",
    "    # 2. Advanced Parameters\n",
    "    'ent_coef_range': (1e-8, 1e-3),          \n",
    "    'vf_coef_range': (0.5, 1.0),             \n",
    "    'n_epochs_range': (5, 15),                \n",
    "    'batch_size_range': (64, 256)             \n",
    "}\n",
    "\n",
    "A2C_PARAMS = {\n",
    "    # Your current parameters\n",
    "    'n_steps_range': (2, 30),                  # Small steps, more frequent updates\n",
    "    'gamma_range': (0.9, 0.9999),               # General discount range\n",
    "    'learning_rate_range': (1e-9, 1e-3),       # Higher learning rates typically better\n",
    "    'ent_coef_range': (1e-8, 1e-3),           # Entropy coefficient for exploration\n",
    "    'vf_coef_range': (0.2, 1.0),              # Value function coefficient   \n",
    "    'gae_lambda_range': (0.9, 0.98),           # Generalized Advantage Estimation lambda           \n",
    "}\n",
    "\n",
    "DQN_PARAMS = {\n",
    "    'buffer_size_range': (20000, 40000),         # Balanced for 84x84x3 RGB observations\n",
    "    'gamma_range': (0.95, 0.9999),                 # Slightly lower gamma since rewards are score-based\n",
    "    'learning_rate_range': (1e-5, 5e-5),         # Lower learning rate for stability with image inputs\n",
    "    'batch_size_range': (32, 64),                # Smaller batches for image processing\n",
    "    'train_freq_range': (4, 8),                  # Update frequently to capture fighting game dynamics\n",
    "    'target_update_interval_range': (1000, 3000), # Regular target updates\n",
    "    'exploration_fraction_range': (0.4, 0.6),     # Longer exploration for 12 possible actions\n",
    "    'exploration_final_eps_range': (0.08, 0.12),  # Higher final exploration due to action space\n",
    "    'learning_starts_range': (10000, 20000)       # More initial experience for image-based \n",
    "}\n",
    "\n",
    "# Define the optimization function for PPO\n",
    "def optimize_ppo(trial):\n",
    "    # Parameter Selection Logic\n",
    "    params = {\n",
    "        'n_steps': trial.suggest_categorical('n_steps', range(PPO_PARAMS['n_steps_range'][0], PPO_PARAMS['n_steps_range'][1], 64)),\n",
    "        'gamma': trial.suggest_loguniform('gamma', *PPO_PARAMS['gamma_range']),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', *PPO_PARAMS['learning_rate_range']),\n",
    "        'clip_range': trial.suggest_uniform('clip_range', *PPO_PARAMS['clip_range_range']),\n",
    "        'gae_lambda': trial.suggest_uniform('gae_lambda', *PPO_PARAMS['gae_lambda_range']),\n",
    "        'ent_coef': trial.suggest_loguniform('ent_coef', *PPO_PARAMS['ent_coef_range']),\n",
    "        'vf_coef': trial.suggest_uniform('vf_coef',  *PPO_PARAMS['vf_coef_range']),\n",
    "        'n_epochs': trial.suggest_int('n_epochs', *PPO_PARAMS['n_epochs_range']),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [64, 128, 256]),\n",
    "        \n",
    "        # Fixed Parameters (Stability Controls)\n",
    "        'max_grad_norm': 0.5,          # Prevents explosive gradients\n",
    "        # 'clip_range_vf': None,         # Uses same clipping as policy\n",
    "        # 'target_kl': None,             # No KL divergence target\n",
    "    }\n",
    "    return params\n",
    "\n",
    "def optimize_a2c(trial):\n",
    "    return {\n",
    "        'n_steps': trial.suggest_int('n_steps', *A2C_PARAMS['n_steps_range']),\n",
    "        'gamma': trial.suggest_loguniform('gamma', *A2C_PARAMS['gamma_range']),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', *A2C_PARAMS['learning_rate_range']),\n",
    "        'ent_coef': trial.suggest_loguniform('ent_coef', *A2C_PARAMS['ent_coef_range']),\n",
    "        'vf_coef': trial.suggest_uniform('vf_coef', *A2C_PARAMS['vf_coef_range']),\n",
    "        'gae_lambda': trial.suggest_uniform('gae_lambda', *A2C_PARAMS['gae_lambda_range']),\n",
    "        # Fixed Parameters (Stability Controls)\n",
    "        'max_grad_norm': 0.5,          # Prevents explosive gradients\n",
    "    }\n",
    "\n",
    "def optimize_dqn(trial):\n",
    "    return {\n",
    "        'buffer_size': trial.suggest_int('buffer_size', *DQN_PARAMS['buffer_size_range']),\n",
    "        'gamma': trial.suggest_float('gamma', *DQN_PARAMS['gamma_range']),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', *DQN_PARAMS['learning_rate_range'], log=True),\n",
    "        'batch_size': trial.suggest_int('batch_size', *DQN_PARAMS['batch_size_range']),\n",
    "        'train_freq': trial.suggest_int('train_freq', *DQN_PARAMS['train_freq_range']),\n",
    "        'target_update_interval': trial.suggest_int('target_update_interval', \n",
    "                                                  *DQN_PARAMS['target_update_interval_range']),\n",
    "        'exploration_fraction': trial.suggest_float('exploration_fraction', \n",
    "                                                  *DQN_PARAMS['exploration_fraction_range']),\n",
    "        'exploration_final_eps': trial.suggest_float('exploration_final_eps', \n",
    "                                                   *DQN_PARAMS['exploration_final_eps_range']),\n",
    "        'learning_starts': trial.suggest_int('learning_starts', *DQN_PARAMS['learning_starts_range'])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = os.path.join(OPT_DIR, 'trial_{}_best_model'.format(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALGORITHMS = {\n",
    "    'PPO': (PPO, optimize_ppo),\n",
    "    'A2C': (A2C, optimize_a2c),\n",
    "    'DQN': (DQN, optimize_dqn),\n",
    "}\n",
    "\n",
    "def optimize_agent(trial, algo_name='PPO'):\n",
    "    try:\n",
    "        # Print trial start\n",
    "        print(f\"\\nStarting Trial {trial.number}\")\n",
    "        \n",
    "        # Select algorithm and get hyperparameters\n",
    "        ModelClass, optimize_fn = ALGORITHMS[algo_name]\n",
    "        model_params = optimize_fn(trial)\n",
    "\n",
    "        # Create environment with error checking\n",
    "        try:\n",
    "            env = StreetFighter()\n",
    "        except Exception as e:\n",
    "            print(f\"Environment creation failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        try:\n",
    "            env = Monitor(env, LOG_DIR)\n",
    "            env = DummyVecEnv([lambda: env])\n",
    "            env = VecFrameStack(env, 4, channels_order='last')\n",
    "        except Exception as e:\n",
    "            print(f\"Environment wrapper failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        # Initialize model with error checking\n",
    "        try:\n",
    "            model = ModelClass('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, **model_params)\n",
    "            print(f\"Model initialized on device: {model.device}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Model initialization failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        # Training with error checking\n",
    "        try:\n",
    "            model.learn(total_timesteps=100000)\n",
    "            print(f\"Training completed for trial {trial.number}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Training failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        # Evaluation with error checking\n",
    "        try:\n",
    "            mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "            print(f\"Evaluation completed with mean reward: {mean_reward}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Evaluation failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        env.close()\n",
    "        \n",
    "        # Save model\n",
    "        SAVE_PATH = os.path.join(OPT_DIR, f'trial_{trial.number}_best_model')\n",
    "        model.save(SAVE_PATH)\n",
    "        \n",
    "        return mean_reward\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nTrial {trial.number} failed with error:\\n{str(e)}\\n\")\n",
    "        print(f\"Error type: {type(e)}\")\n",
    "        import traceback\n",
    "        print(traceback.format_exc())\n",
    "        return -1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALGO = 'PPO' # A2C, DQN, PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the experiment \n",
    "study = optuna.create_study(direction='maximize') # since mean reward is positive we maximize, otherwise minimize\n",
    "study.optimize(lambda trial: optimize_agent(trial, algo_name=ALGO), n_trials=25) # previously 50; 10 is a demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import base callback \n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback): # continuously learn by starting from best parameters done above\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=5000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment \n",
    "env = StreetFighter()\n",
    "env = Monitor(env, LOG_DIR)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, 4, channels_order='last')\n",
    "\n",
    "# # loading model from zip (temp, comment out later)\n",
    "# model_path = os.path.join(OPT_DIR, 'trial_4_best_model')\n",
    "# model_from_zip = PPO.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_factor_64_round_down(num):\n",
    "    return num - (num % 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loading model params from zip (temp, comment out layer)\n",
    "# model_params = {\n",
    "#     'n_steps': model_from_zip.n_steps,           \n",
    "#     'gamma': model_from_zip.gamma,          \n",
    "#     'learning_rate': model_from_zip.learning_rate,   \n",
    "#     'clip_range': model_from_zip.clip_range,        \n",
    "#     'gae_lambda': model_from_zip.gae_lambda,  \n",
    "#     'ent_coef': model_from_zip.ent_coef,        \n",
    "#     'vf_coef': model_from_zip.vf_coef,            \n",
    "#     'n_epochs': model_from_zip.n_epochs,              \n",
    "#     'batch_size': model_from_zip.batch_size,\n",
    "# }\n",
    "model_params = study.best_params\n",
    "if ALGO == 'PPO':\n",
    "    model_params['n_steps'] = closest_factor_64_round_down(model_params['n_steps'])\n",
    "# model_params['learning_rate'] = 5e-7 -> if really slow at training\n",
    "model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "if ALGO == 'PPO':\n",
    "    model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, **model_params)\n",
    "elif ALGO == 'A2C':\n",
    "    model = A2C('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, **model_params)\n",
    "else:\n",
    "    model = DQN('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, **model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload previous weights from HPO\n",
    "best_trial = study.best_trial.number\n",
    "model.load(os.path.join(OPT_DIR, 'trial_{}_best_model.zip').format(best_trial))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_TIMESTEPS = 10000000 # previously 5 mil; 100k is demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kick off training \n",
    "model.learn(total_timesteps=TRAINING_TIMESTEPS, callback=callback) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard --logdir=. \n",
    "# cd to logs\n",
    "# ^ use to visually see learning progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ALGO == 'PPO':\n",
    "    model = PPO.load(f'./train/best_model_{TRAINING_TIMESTEPS}.zip')\n",
    "elif ALGO == 'A2C':\n",
    "    model = A2C.load(f'./train/best_model_{TRAINING_TIMESTEPS}.zip')\n",
    "else:\n",
    "    model = DQN.load(f'./train/best_model_{TRAINING_TIMESTEPS}.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, _ = evaluate_policy(model, env, render=True, n_eval_episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(model.predict(obs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset game to starting state\n",
    "obs = env.reset()\n",
    "# Set flag to flase\n",
    "done = False\n",
    "for game in range(1): \n",
    "    while not done: \n",
    "        if done: \n",
    "            obs = env.reset()\n",
    "        env.render()\n",
    "        action = model.predict(obs)[0]\n",
    "        print(action)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        time.sleep(0.01)\n",
    "        # print(reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
